{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import random\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "import pandas as pd\n",
    "from shapely.geometry import MultiLineString, LineString\n",
    "import ortools\n",
    "from shapely.ops import nearest_points\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import multiprocessing\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LINK 파일 불러오기\n",
    "link_df=gpd.read_file(\".\\[2024-03-25]NODELINKDATA\\MOCT_LINK.shp\",encoding='euc-kr')\n",
    "link_df['geometry'] = link_df['geometry'].to_crs('EPSG:4326') # 위도경도 좌표계 변환\n",
    "\n",
    "# 새로운 DBF 파일 읽기\n",
    "new_df = gpd.read_file(\"2022-TM-GR-MR-LLV2 도로망(2021년 기준_세계)\\\\2022-TM-GR-MR-LLV2 도로망(2021년 기준)\\\\02. 링크\\\\ad0022_2021_GR.dbf\")\n",
    "new_df['geometry'] = new_df['geometry'].to_crs('EPSG:4326') # 위도경도 좌표계 변환\n",
    "\n",
    "link_columns_to_drop = ['UPDATEDATE', 'REMARK', 'HIST_TYPE', 'HISTREMARK','REST_VEH', 'REST_W','REST_H','REMARK','MULTI_LINK','CONNECT','C-ITS']\n",
    "link_df = link_df.drop(columns=link_columns_to_drop, errors='ignore')\n",
    "\n",
    "tempL=link_df['LINK_ID'].apply(lambda x:int(x[:3]))\n",
    "slink_df=link_df[(tempL>=100)&(tempL<=124)] ### 서울 코드 100~124\n",
    "\n",
    "# 국토교통부_전국 노드링크별 평균택시속도_20231231 (1) 파일 불러오기\n",
    "taxi_speed_df = pd.read_csv(\".\\국토교통부_전국 노드링크별 평균택시속도_20231231 (1).csv\")\n",
    "\n",
    "# '링크아이디' 열 이름을 'LINK_ID'로 변경\n",
    "taxi_speed_df = taxi_speed_df.rename(columns={'링크아이디': 'LINK_ID'})\n",
    "\n",
    "# 데이터 타입을 일치시키기 위해 LINK_ID를 문자열로 변환\n",
    "slink_df['LINK_ID'] = slink_df['LINK_ID'].astype(str)\n",
    "taxi_speed_df['LINK_ID'] = taxi_speed_df['LINK_ID'].astype(str)\n",
    "\n",
    "# 필요한 열만 남기기: LINK_ID와 평균속도\n",
    "taxi_speed_df = taxi_speed_df[['LINK_ID', '평균속도']]\n",
    "\n",
    "# slink_df와 taxi_speed_df를 LINK_ID를 기준으로 병합\n",
    "slink_df = slink_df.merge(taxi_speed_df, on='LINK_ID', how='left')\n",
    "\n",
    "# 병합 결과 확인\n",
    "slink_df.head()\n",
    "\n",
    "\n",
    "# NODE 파일 불러오기\n",
    "\n",
    "node_df = gpd.read_file(\".\\[2024-03-25]NODELINKDATA\\MOCT_NODE.shp\", encoding='euc-kr')\n",
    "node_df['geometry'] = node_df['geometry'].to_crs('EPSG:4326')  # 위도경도 좌표계 변환\n",
    "\n",
    "node_columns_to_drop = ['UPDATEDATE', 'REMARK', 'HIST_TYPE', 'HISTREMARK']\n",
    "node_df = node_df.drop(columns=node_columns_to_drop, errors='ignore')\n",
    "\n",
    "tempN=node_df['NODE_ID'].apply(lambda x:int(x[:3]))\n",
    "snode_df=node_df[(tempN>=100)&(tempN<=124)] ### 서울 코드 100~124\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link 파일 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#교통망 GIS DB와 ITS 국가교통정보센터에서 쓰는 LINK_ID 일치화\n",
    "'''\n",
    "def find_nearest_geometry(target_geom, geometries):\n",
    "    if target_geom is None:\n",
    "        return None\n",
    "    nearest_geom = min(geometries, key=lambda geom: target_geom.distance(geom) if geom is not None else float('inf'))\n",
    "    return nearest_geom\n",
    "\n",
    "# 완전히 일치하는 geometry 찾기\n",
    "exact_match = gpd.sjoin(slink_df, new_df[['geometry', 'TL_DENSITY']], how='inner', predicate='within')\n",
    "exact_match = exact_match[exact_match.apply(lambda row: row['geometry'].equals(new_df.loc[row['index_right'], 'geometry']), axis=1)]\n",
    "\n",
    "# 일치하지 않는 geometry에 대해 가장 가까운 geometry 찾기\n",
    "unmatched = slink_df[~slink_df['LINK_ID'].isin(exact_match['LINK_ID'])]\n",
    "nearest_match = gpd.sjoin_nearest(unmatched, new_df[['geometry', 'TL_DENSITY']], how='left', max_distance=1000)  # max_distance 추가\n",
    "\n",
    "# 중복 제거 (LINK_ID 기준으로 첫 번째 매치만 유지)\n",
    "nearest_match = nearest_match.groupby('LINK_ID').first().reset_index()\n",
    "\n",
    "# 결과 합치기\n",
    "merged_df = pd.concat([exact_match, nearest_match])\n",
    "\n",
    "# 중복 열 제거 및 인덱스 재설정\n",
    "merged_df = merged_df.drop(columns=['index_right'], errors='ignore').reset_index(drop=True)\n",
    "\n",
    "# NaN 값 처리 (선택적)\n",
    "merged_df = merged_df.dropna(subset=['geometry'])\n",
    "\n",
    "# 결과를 slink_df에 할당\n",
    "slink_df = merged_df\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#빈 평균속도 값 NODE 꼬리물면서 찾기\n",
    "\n",
    "def fill_missing_speed_recursive(df, link_id, visited=None, depth=0):\n",
    "    if visited is None:\n",
    "        visited = set()\n",
    "    \n",
    "    if link_id in visited or depth > 10:  # 깊이 제한 추가\n",
    "        return None\n",
    "    \n",
    "    visited.add(link_id)\n",
    "    row = df[df['LINK_ID'] == link_id].iloc[0]\n",
    "    \n",
    "    if pd.notnull(row['평균속도']):\n",
    "        return row['평균속도']\n",
    "    \n",
    "    # F_NODE와 일치하는 T_NODE를 가진 행들 찾기\n",
    "    matching_rows_f = df[(df['T_NODE'] == row['F_NODE']) & (df['LINK_ID'] != link_id)]\n",
    "    \n",
    "    for _, matching_row in matching_rows_f.iterrows():\n",
    "        speed = fill_missing_speed_recursive(df, matching_row['LINK_ID'], visited, depth + 1)\n",
    "        if speed is not None:\n",
    "            return speed\n",
    "    \n",
    "    # T_NODE와 일치하는 F_NODE를 가진 행들 찾기\n",
    "    matching_rows_t = df[(df['F_NODE'] == row['T_NODE']) & (df['LINK_ID'] != link_id)]\n",
    "    \n",
    "    for _, matching_row in matching_rows_t.iterrows():\n",
    "        speed = fill_missing_speed_recursive(df, matching_row['LINK_ID'], visited, depth + 1)\n",
    "        if speed is not None:\n",
    "            return speed\n",
    "    \n",
    "    return None\n",
    "\n",
    "def fill_missing_speed(df):\n",
    "    for index, row in df[df['평균속도'].isnull()].iterrows():\n",
    "        speed = fill_missing_speed_recursive(df, row['LINK_ID'])\n",
    "        if speed is not None:\n",
    "            df.at[index, '평균속도'] = speed\n",
    "    return df\n",
    "\n",
    "# slink_df에 함수 적용\n",
    "slink_df = fill_missing_speed(slink_df)\n",
    "\n",
    "# 결과 확인\n",
    "print(slink_df[['LINK_ID', 'F_NODE', 'T_NODE', 'ROAD_RANK', '평균속도']])\n",
    "\n",
    "# 남아있는 null 값 확인\n",
    "null_count = slink_df['평균속도'].isnull().sum()\n",
    "print(f\"남아있는 null 값의 개수: {null_count}\")\n",
    "\n",
    "# 남아있는 null 값에 대해 ROAD_RANK별 평균 속도 적용\n",
    "if null_count > 0:\n",
    "    mean_speed_by_rank = slink_df.groupby('ROAD_RANK')['평균속도'].mean()\n",
    "    \n",
    "    for index, row in slink_df[slink_df['평균속도'].isnull()].iterrows():\n",
    "        slink_df.at[index, '평균속도'] = mean_speed_by_rank[row['ROAD_RANK']]\n",
    "\n",
    "    # 최종 결과 확인\n",
    "    print(\"ROAD_RANK별 평균 속도 적용 후:\")\n",
    "    print(slink_df[['LINK_ID', 'F_NODE', 'T_NODE', 'ROAD_RANK', '평균속도']])\n",
    "    \n",
    "    # 최종 null 값 확인\n",
    "    final_null_count = slink_df['평균속도'].isnull().sum()\n",
    "    print(f\"최종 남아있는 null 값의 개수: {final_null_count}\")\n",
    "    '''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#파라미터 적용\n",
    "'''\n",
    "#vdr_parameter.CSV 파일 읽기\n",
    "vdr_params = pd.read_csv('vdr_parameter.CSV')\n",
    "\n",
    "# 숫자 컬럼의 쉼표 제거 및 숫자형으로 변환\n",
    "numeric_columns = ['C_L', 'C_S', 'C_U', 'ROAD_RANK_L', 'ROAD_RANK_U', 'LANES_L', 'LANES_U', 'TL_DENSITY_L', 'TL_DENSITY_U']\n",
    "for col in numeric_columns:\n",
    "    vdr_params[col] = pd.to_numeric(vdr_params[col].astype(str).str.replace(',', ''), errors='coerce')\n",
    "\n",
    "# link_df의 관련 열들을 숫자형으로 변환\n",
    "slink_df['ROAD_RANK'] = pd.to_numeric(slink_df['ROAD_RANK'], errors='coerce')\n",
    "slink_df['LANES'] = pd.to_numeric(slink_df['LANES'], errors='coerce')\n",
    "slink_df['TL_DENSITY'] = pd.to_numeric(slink_df['TL_DENSITY'], errors='coerce')\n",
    "\n",
    "# 조건에 따라 VDF 값을 할당하는 함수\n",
    "def assign_vdf(row, vdr_params):\n",
    "    for _, vdr_row in vdr_params.iterrows():\n",
    "        road_rank_condition = vdr_row['ROAD_RANK_L'] < row['ROAD_RANK'] <= vdr_row['ROAD_RANK_U']\n",
    "        lanes_condition = vdr_row['LANES_L'] < row['LANES'] <= vdr_row['LANES_U']\n",
    "        \n",
    "        if row['ROAD_RANK'] in [101, 102]:\n",
    "            # ROAD_RANK가 101 또는 102인 경우 TL_DENSITY 조건을 무시\n",
    "            if road_rank_condition and lanes_condition:\n",
    "                return pd.Series({\n",
    "                    'VDF': vdr_row['VDF'],\n",
    "                    'BPR_A': vdr_row['BPR_A'],\n",
    "                    'BPR_B': vdr_row['BPR_B'],\n",
    "                    'V_L': vdr_row['V_L'],\n",
    "                    'V_S': vdr_row['V_S'],\n",
    "                    'V_U': vdr_row['V_U'],\n",
    "                    'C_L': vdr_row['C_L'],\n",
    "                    'C_S': vdr_row['C_S'],\n",
    "                    'C_U': vdr_row['C_U']\n",
    "                })\n",
    "        else:\n",
    "            # TL_DENSITY가 0인 경우 특별 처리\n",
    "            if row['TL_DENSITY'] == 0:\n",
    "                tl_density_condition = vdr_row['TL_DENSITY_L'] == 0 and vdr_row['TL_DENSITY_U'] == 0.3\n",
    "            elif pd.isna(row.get('TL_DENSITY')):\n",
    "                tl_density_condition = True  # TL_DENSITY가 NaN이면 조건을 무시\n",
    "            else:\n",
    "                tl_density_condition = vdr_row['TL_DENSITY_L'] < row['TL_DENSITY'] <= vdr_row['TL_DENSITY_U']\n",
    "            \n",
    "            if road_rank_condition and lanes_condition and tl_density_condition:\n",
    "                return pd.Series({\n",
    "                    'VDF': vdr_row['VDF'],\n",
    "                    'BPR_A': vdr_row['BPR_A'],\n",
    "                    'BPR_B': vdr_row['BPR_B'],\n",
    "                    'V_L': vdr_row['V_L'],\n",
    "                    'V_S': vdr_row['V_S'],\n",
    "                    'V_U': vdr_row['V_U'],\n",
    "                    'C_L': vdr_row['C_L'],\n",
    "                    'C_S': vdr_row['C_S'],\n",
    "                    'C_U': vdr_row['C_U']\n",
    "                })\n",
    "    \n",
    "    # 조건에 맞는 값이 없을 경우 기본값 반환\n",
    "    return pd.Series({\n",
    "        'VDF': None,\n",
    "        'BPR_A': None,\n",
    "        'BPR_B': None,\n",
    "        'V_L': None,\n",
    "        'V_S': None,\n",
    "        'V_U': None,\n",
    "        'C_L': None,\n",
    "        'C_S': None,\n",
    "        'C_U': None\n",
    "    })\n",
    "\n",
    "# slink_df에 VDF 값 할당\n",
    "new_columns = slink_df.apply(lambda row: assign_vdf(row, vdr_params), axis=1)\n",
    "slink_df = pd.concat([slink_df, new_columns], axis=1)\n",
    "\n",
    "display(slink_df)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# slink_df를 CSV 파일로 저장\n",
    "slink_df.to_csv(\"slink_df_output.csv\", index=False, encoding='utf-8-sig')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LinK 시각화 함수\n",
    "\n",
    "def linechart(df):\n",
    "    m = folium.Map(location=[37.5665, 126.9780], zoom_start=15)\n",
    "    for _, row in df.iterrows():\n",
    "        if isinstance(row['geometry'], MultiLineString) and len(row['geometry'].geoms) > 1:\n",
    "            for line_string in row['geometry'].geoms:\n",
    "                coordinates = [(lat, lon) for lon, lat in line_string.coords]\n",
    "                folium.PolyLine(locations=coordinates, color='blue').add_to(m)\n",
    "        else:\n",
    "            coordinates = [(lat, lon) for lon, lat in zip(*row['geometry'].xy)]\n",
    "            folium.PolyLine(locations=coordinates, color='blue').add_to(m)\n",
    "    return m\n",
    "\n",
    "linechart(slink_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "display(snode_df)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# NODE 시각화 함수\n",
    "def node_chart(df):\n",
    "    m = folium.Map(location=[37.5665, 126.9780], zoom_start=11)\n",
    "    for _, row in df.iterrows():\n",
    "        folium.CircleMarker(\n",
    "            location=[row.geometry.y, row.geometry.x],\n",
    "            radius=2,\n",
    "            popup=row['NODE_ID'],\n",
    "            color='red',\n",
    "            fill=True\n",
    "        ).add_to(m)\n",
    "    return m\n",
    "\n",
    "# 서울 지역 NODE 시각화\n",
    "seoul_node_map = node_chart(snode_df)\n",
    "display(seoul_node_map)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# 노드와 링크를 겹쳐서 시각화\n",
    "def visualize_nodes_and_links(node_df, link_df):\n",
    "    m = folium.Map(location=[37.5665, 126.9780], zoom_start=15)\n",
    "    \n",
    "    # Add links to the map\n",
    "    for _, row in link_df.iterrows():\n",
    "        if isinstance(row['geometry'], MultiLineString) and len(row['geometry'].geoms) > 1:\n",
    "            for line_string in row['geometry'].geoms:\n",
    "                coordinates = [(lat, lon) for lon, lat in line_string.coords]\n",
    "                folium.PolyLine(locations=coordinates, color='blue').add_to(m)\n",
    "        else:\n",
    "            coordinates = [(lat, lon) for lon, lat in zip(*row['geometry'].xy)]\n",
    "            folium.PolyLine(locations=coordinates, color='blue').add_to(m)\n",
    "    \n",
    "    # Add nodes to the map\n",
    "    for _, row in node_df.iterrows():\n",
    "        folium.CircleMarker(\n",
    "            location=[row.geometry.y, row.geometry.x],\n",
    "            radius=2,\n",
    "            popup=row['NODE_ID'],\n",
    "            color='red',\n",
    "            fill=True\n",
    "        ).add_to(m)\n",
    "    \n",
    "    return m\n",
    "\n",
    "# 시각화\n",
    "combined_map = visualize_nodes_and_links(snode_df, slink_df)\n",
    "display(combined_map)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BPR 함수를 이용해서 교통량 추정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def calculate_volume(row):\n",
    "    C = row['C_S']\n",
    "    alpha = row['BPR_A']\n",
    "    beta = row['BPR_B']\n",
    "    t = row['LENGTH'] / row['평균속도']\n",
    "    t_0 = row['LENGTH'] / row['V_U']\n",
    "    \n",
    "    # BPR 함수를 이용하여 V 계산\n",
    "    if t <= t_0:\n",
    "        return 0  # 현재 통행시간이 자유 통행시간보다 작거나 같으면 교통량은 0\n",
    "    \n",
    "    V = C * ((t / t_0 - 1) / alpha) ** (1 / beta)\n",
    "    return V\n",
    "\n",
    "# slink_df에 새로운 열 'Calculated_Volume' 추가\n",
    "slink_df['Calculated_Volume'] = slink_df.apply(calculate_volume, axis=1)\n",
    "\n",
    "# 결과 확인\n",
    "print(slink_df[['LINK_ID', 'LENGTH', '평균속도', 'V_S', 'C_S', 'BPR_A', 'BPR_B', 'Calculated_Volume']])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "링크에 구명 배정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "slink_df.to_csv(\"slink_df_output_with_q.csv\", index=False, encoding='utf-8-sig')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# 필요한 라이브러리 import\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# 도로명코드_전체 파일 읽기 \n",
    "road_code_df = pd.read_csv(\"도로명코드_전체.csv\", encoding='euc-kr')\n",
    "\n",
    "# slink_df 읽기 (이미 있다고 가정)\n",
    "# slink_df = gpd.read_file(\"path_to_slink_file.shp\")\n",
    "\n",
    "# 도로명과 시군구명 매칭 함수\n",
    "def match_road_name(row, road_code_df):\n",
    "    matched = road_code_df[road_code_df['도로명'] == row['ROAD_NAME']]\n",
    "    if not matched.empty:\n",
    "        return matched.iloc[0]['시군구명']\n",
    "    return None\n",
    "\n",
    "# 연결된 링크를 통해 시군구명 찾기 함수\n",
    "def find_sigungu_recursive(df, link_id, visited=None, depth=0):\n",
    "    if visited is None:\n",
    "        visited = set()\n",
    "    \n",
    "    if link_id in visited or depth > 10:  # 깊이 제한\n",
    "        return None\n",
    "    \n",
    "    visited.add(link_id)\n",
    "    row = df[df['LINK_ID'] == link_id].iloc[0]\n",
    "    \n",
    "    if pd.notnull(row['시군구명']):\n",
    "        return row['시군구명']\n",
    "    \n",
    "    # F_NODE와 일치하는 T_NODE를 가진 행들 찾기\n",
    "    matching_rows_f = df[(df['T_NODE'] == row['F_NODE']) & (df['LINK_ID'] != link_id)]\n",
    "    \n",
    "    for _, matching_row in matching_rows_f.iterrows():\n",
    "        sigungu = find_sigungu_recursive(df, matching_row['LINK_ID'], visited, depth + 1)\n",
    "        if sigungu is not None:\n",
    "            return sigungu\n",
    "    \n",
    "    # T_NODE와 일치하는 F_NODE를 가진 행들 찾기\n",
    "    matching_rows_t = df[(df['F_NODE'] == row['T_NODE']) & (df['LINK_ID'] != link_id)]\n",
    "    \n",
    "    for _, matching_row in matching_rows_t.iterrows():\n",
    "        sigungu = find_sigungu_recursive(df, matching_row['LINK_ID'], visited, depth + 1)\n",
    "        if sigungu is not None:\n",
    "            return sigungu\n",
    "    \n",
    "    return None\n",
    "\n",
    "# 도로명 매칭 적용\n",
    "slink_df['시군구명'] = slink_df.apply(lambda row: match_road_name(row, road_code_df), axis=1)\n",
    "\n",
    "# 연결된 링크를 통해 시군구명 찾기\n",
    "for index, row in slink_df[slink_df['시군구명'].isnull()].iterrows():\n",
    "    sigungu = find_sigungu_recursive(slink_df, row['LINK_ID'])\n",
    "    if sigungu is not None:\n",
    "        slink_df.at[index, '시군구명'] = sigungu\n",
    "\n",
    "# 결과 확인\n",
    "print(slink_df[['LINK_ID', 'ROAD_NAME', '시군구명']])\n",
    "\n",
    "# 남아있는 null 값 확인\n",
    "null_count = slink_df['시군구명'].isnull().sum()\n",
    "print(f\"시군구명이 없는 링크 수: {null_count}\")\n",
    "slink_df.to_csv(\"slink_df_output_with_q,s.csv\", index=False, encoding='utf-8-sig')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Slink_df_output.csv 파일 읽기\n",
    "slink_df = pd.read_csv('Slink_df_output_with_q,s.csv')\n",
    "\n",
    "# 만약 geometry 열이 있다면, 이를 GeoSeries로 변환\n",
    "if 'geometry' in slink_df.columns:\n",
    "    slink_df['geometry'] = gpd.GeoSeries.from_wkt(slink_df['geometry'])\n",
    "    slink_df = gpd.GeoDataFrame(slink_df, geometry='geometry')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import numpy as np\n",
    "from scipy.optimize import fsolve\n",
    "\n",
    "# 금천구 링크만 선택\n",
    "geumcheon_links = slink_df[slink_df['시군구명'] == '금천구']\n",
    "\n",
    "def bpr_inverse(speed, row):\n",
    "    t = row['LENGTH'] / speed\n",
    "    t_0 = row['LENGTH'] / row['V_U']\n",
    "    C = row['C_S']\n",
    "    V = row['Calculated_Volume'] * 0.67  # 67% of the current volume\n",
    "    alpha = row['BPR_A']\n",
    "    beta = row['BPR_B']\n",
    "    \n",
    "    return t / t_0 - (1 + alpha * (V / C) ** beta)\n",
    "\n",
    "def calculate_new_speed(row):\n",
    "    initial_guess = row['평균속도']\n",
    "    new_speed = fsolve(bpr_inverse, initial_guess, args=(row,))[0]\n",
    "    return new_speed\n",
    "\n",
    "# 새로운 속도 계산\n",
    "geumcheon_links['New_Speed'] = geumcheon_links.apply(calculate_new_speed, axis=1)\n",
    "\n",
    "# 결과 출력\n",
    "print(geumcheon_links[['LINK_ID', '평균속도', 'New_Speed', 'Calculated_Volume']])\n",
    "\n",
    "# 평균 속도 변화 계산\n",
    "avg_speed_change = (geumcheon_links['New_Speed'].mean() - geumcheon_links['평균속도'].mean()) / geumcheon_links['평균속도'].mean() * 100\n",
    "\n",
    "print(f\"\\n평균 속도 변화: {avg_speed_change:.2f}%\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import pandas as pd\n",
    "import geopandas as gpd\n",
    "import networkx as nx\n",
    "from shapely.geometry import LineString\n",
    "import numpy as np\n",
    "\n",
    "# 1. 데이터 로드 및 준비\n",
    "geumcheon_links = pd.read_csv('geumcheon_links.csv')\n",
    "gdf = gpd.GeoDataFrame(geumcheon_links, geometry=gpd.GeoSeries.from_wkt(geumcheon_links['geometry']))\n",
    "\n",
    "def distribute_flow_ue(G, paths, total_flow, max_iterations=100, convergence_threshold=0.01):\n",
    "\n",
    "    n_paths = len(paths)\n",
    "    flows = np.full(n_paths, total_flow / n_paths)  # 초기에는 균등하게 분배\n",
    "    \n",
    "    for _ in range(max_iterations):\n",
    "        old_flows = flows.copy()\n",
    "        times = np.array([calculate_travel_time(G, path, flow) for path, flow in zip(paths, flows)])\n",
    "        \n",
    "        # 최소 통행 시간을 가진 경로로 통행량을 이동\n",
    "        min_time = np.min(times)\n",
    "        flow_to_move = total_flow * 0.1  # 한 번에 이동할 통행량의 비율\n",
    "        \n",
    "        for i in range(n_paths):\n",
    "            if times[i] > min_time:\n",
    "                flows[i] = max(0, flows[i] - flow_to_move)\n",
    "            else:\n",
    "                flows[i] += flow_to_move\n",
    "        \n",
    "        # 총 통행량 유지\n",
    "        flows = flows * (total_flow / np.sum(flows))\n",
    "        \n",
    "        # 수렴 확인\n",
    "        if np.max(np.abs(flows - old_flows)) < convergence_threshold:\n",
    "            break\n",
    "    \n",
    "    return flows\n",
    "\n",
    "# 2. 네트워크 그래프 생성\n",
    "G = nx.DiGraph()\n",
    "\n",
    "for idx, row in gdf.iterrows():\n",
    "    G.add_edge(row['F_NODE'], row['T_NODE'], \n",
    "               length=row['LENGTH'], \n",
    "               speed=row['New_Speed'],\n",
    "               link_id=row['LINK_ID'],\n",
    "               capacity=row['C_U'],\n",
    "               bpr_a=row['BPR_A'],\n",
    "               bpr_b=row['BPR_B'])\n",
    "\n",
    "def calculate_travel_time(G, path, additional_flow=0):\n",
    "    travel_time = 0\n",
    "    for i in range(len(path) - 1):\n",
    "        if G.has_edge(path[i], path[i+1]):\n",
    "            edge = G[path[i]][path[i+1]]\n",
    "            link_data = gdf[gdf['LINK_ID'] == edge['link_id']].iloc[0]\n",
    "            length = edge['length']\n",
    "            speed = edge['speed']\n",
    "            capacity = edge['capacity']\n",
    "            volume = link_data['Calculated_Volume'] + additional_flow\n",
    "            bpr_a = edge['bpr_a']\n",
    "            bpr_b = edge['bpr_b']\n",
    "            \n",
    "            # BPR 함수\n",
    "            travel_time += length / speed * (1 + bpr_a * (volume / capacity) ** bpr_b)\n",
    "        else:\n",
    "            # 엣지가 존재하지 않으면 큰 값 반환\n",
    "            return float('inf')\n",
    "    \n",
    "    return travel_time\n",
    "\n",
    "def find_alternative_paths(G, start, end, k=2):\n",
    "    try:\n",
    "        paths = list(nx.shortest_simple_paths(G, start, end, weight='length'))\n",
    "        return [path for path in paths[:k] if all(G.has_edge(path[i], path[i+1]) for i in range(len(path)-1))]\n",
    "    except nx.NetworkXNoPath:\n",
    "        return []\n",
    "\n",
    "# 3. 모든 링크에 대해 반복\n",
    "braess_paradox_links = []\n",
    "\n",
    "for idx, link in gdf.iterrows():\n",
    "    start_node = link['F_NODE']\n",
    "    end_node = link['T_NODE']\n",
    "    \n",
    "    # 현재 링크의 초기 통행 시간 계산\n",
    "    initial_time_current = calculate_travel_time(G, [start_node, end_node])\n",
    "    \n",
    "    # 대체 경로 찾기\n",
    "    alternative_paths = find_alternative_paths(G, start_node, end_node)\n",
    "    \n",
    "    if len(alternative_paths) < 1:\n",
    "        print(f\"링크 {link['LINK_ID']}에 대한 대체 경로를 찾을 수 없습니다. 건너뜁니다.\")\n",
    "        continue\n",
    "    \n",
    "    # 대체 경로의 초기 통행 시간 계산\n",
    "    additional_flow = link['Calculated_Volume']\n",
    "    new_flows = distribute_flow_ue(G, alternative_paths, additional_flow)\n",
    "    new_times = [calculate_travel_time(G, path, flow) for path, flow in zip(alternative_paths, new_flows)]\n",
    "    \n",
    "    # 링크 제거\n",
    "    G.remove_edge(start_node, end_node)\n",
    "    \n",
    "    # 교통량 재분배\n",
    "    additional_flow = link['Calculated_Volume'] / len(alternative_paths)\n",
    "    \n",
    "    # 대체 경로의 새로운 통행 시간 계산\n",
    "    new_times = [calculate_travel_time(G, path, additional_flow) for path in alternative_paths]\n",
    "    \n",
    "    # Braess의 역설 확인\n",
    "    if all(new_time < initial_time for new_time, initial_time in zip(new_times, initial_times)):\n",
    "        print(f\"링크 {link['LINK_ID']}에서 Braess의 역설이 관찰되었습니다.\")\n",
    "        braess_paradox_links.append(link['LINK_ID'])\n",
    "    \n",
    "    # 그래프에 링크 다시 추가\n",
    "    G.add_edge(start_node, end_node, \n",
    "               length=link['LENGTH'], \n",
    "               speed=link['New_Speed'],\n",
    "               link_id=link['LINK_ID'],\n",
    "               capacity=link['C_U'],\n",
    "               bpr_a=link['BPR_A'],\n",
    "               bpr_b=link['BPR_B'])\n",
    "\n",
    "print(f\"Braess의 역설을 보이는 링크 수: {len(braess_paradox_links)}\")\n",
    "print(\"Braess의 역설을 보이는 링크:\", braess_paradox_links)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import networkx as nx\n",
    "from shapely.geometry import LineString\n",
    "import numpy as np\n",
    "\n",
    "# 결과를 저장할 리스트 생성\n",
    "results = []\n",
    "\n",
    "# 1. 데이터 로드 및 준비\n",
    "geumcheon_links = pd.read_csv('geumcheon_links.csv')\n",
    "gdf = gpd.GeoDataFrame(geumcheon_links, geometry=gpd.GeoSeries.from_wkt(geumcheon_links['geometry']))\n",
    "\n",
    "def distribute_flow_ue(G, paths, total_flow, max_iterations=100, convergence_threshold=0.01):\n",
    "\n",
    "    n_paths = len(paths)\n",
    "    flows = np.full(n_paths, total_flow / n_paths)  # 초기에는 균등하게 분배\n",
    "\n",
    "    for _ in range(max_iterations):\n",
    "        old_flows = flows.copy()\n",
    "        times = np.array([calculate_travel_time(G, path, flow) for path, flow in zip(paths, flows)])\n",
    "\n",
    "        # 최소 통행 시간을 가진 경로로 통행량을 이동\n",
    "        min_time = np.min(times)\n",
    "        flow_to_move = total_flow * 0.1  # 한 번에 이동할 통행량의 비율\n",
    "\n",
    "        for i in range(n_paths):\n",
    "            if times[i] > min_time:\n",
    "                flows[i] = max(0, flows[i] - flow_to_move)\n",
    "            else:\n",
    "                flows[i] += flow_to_move\n",
    "\n",
    "        # 총 통행량 유지\n",
    "        flows = flows * (total_flow / np.sum(flows))\n",
    "\n",
    "        # 수렴 확인\n",
    "        if np.max(np.abs(flows - old_flows)) < convergence_threshold:\n",
    "            break\n",
    "\n",
    "    return flows\n",
    "\n",
    "# 2. 네트워크 그래프 생성\n",
    "G = nx.DiGraph()\n",
    "\n",
    "for idx, row in gdf.iterrows():\n",
    "    G.add_edge(row['F_NODE'], row['T_NODE'],\n",
    "               length=row['LENGTH'],\n",
    "               speed=row['New_Speed'],\n",
    "               link_id=row['LINK_ID'],\n",
    "               capacity=row['C_U'],\n",
    "               bpr_a=row['BPR_A'],\n",
    "               bpr_b=row['BPR_B'])\n",
    "\n",
    "def calculate_travel_time(G, path, additional_flow=0):\n",
    "    travel_time = 0\n",
    "    for i in range(len(path) - 1):\n",
    "        if G.has_edge(path[i], path[i+1]):\n",
    "            edge = G[path[i]][path[i+1]]\n",
    "            link_data = gdf[gdf['LINK_ID'] == edge['link_id']].iloc[0]\n",
    "            length = edge['length']\n",
    "            speed = edge['speed']\n",
    "            capacity = edge['capacity']\n",
    "            volume = link_data['Calculated_Volume'] + additional_flow\n",
    "            bpr_a = edge['bpr_a']\n",
    "            bpr_b = edge['bpr_b']\n",
    "\n",
    "            # BPR 함수\n",
    "            travel_time += length / speed * (1 + bpr_a * (volume / capacity) ** bpr_b)\n",
    "        else:\n",
    "            # 엣지가 존재하지 않으면 큰 값 반환\n",
    "            return float('inf')\n",
    "\n",
    "    return travel_time\n",
    "\n",
    "\n",
    "def find_alternative_paths(G, start, end, k=4):  # k= 대채경로의 수 \n",
    "    try:\n",
    "        paths = list(nx.shortest_simple_paths(G, start, end, weight='length'))\n",
    "        return [path for path in paths[:k] if all(G.has_edge(path[i], path[i+1]) for i in range(len(path)-1))]\n",
    "    except nx.NetworkXNoPath:\n",
    "        return []\n",
    "\n",
    "braess_paradox_links = []\n",
    "\n",
    "for idx, link in gdf.iterrows():\n",
    "    start_node = link['F_NODE']\n",
    "    end_node = link['T_NODE']\n",
    "\n",
    "    # 현재 링크의 초기 통행 시간 계산\n",
    "    initial_time_current = calculate_travel_time(G, [start_node, end_node])\n",
    "\n",
    "    # 대체 경로 찾기\n",
    "    alternative_paths = find_alternative_paths(G, start_node, end_node)  # k=4가 기본값으로 설정됨\n",
    "\n",
    "    if len(alternative_paths) < 1:\n",
    "        print(f\"링크 {link['LINK_ID']}에 대한 대체 경로를 찾을 수 없습니다. 건너뜁니다.\")\n",
    "        continue\n",
    "\n",
    "    # 대체 경로의 초기 통행 시간 계산\n",
    "    additional_flow = link['Calculated_Volume']\n",
    "    initial_times = [calculate_travel_time(G, path, 0) for path in alternative_paths]\n",
    "\n",
    "    new_flows = distribute_flow_ue(G, alternative_paths, additional_flow)\n",
    "    new_times = [calculate_travel_time(G, path, flow) for path, flow in zip(alternative_paths, new_flows)]\n",
    "\n",
    "    # 링크 제거\n",
    "    G.remove_edge(start_node, end_node)\n",
    "\n",
    "    # 교통량 재분배\n",
    "    additional_flow = link['Calculated_Volume'] / len(alternative_paths)\n",
    "\n",
    "    # 대체 경로의 새로운 통행 시간 계산\n",
    "    new_times = [calculate_travel_time(G, path, additional_flow) for path in alternative_paths]\n",
    "\n",
    "    # Braess의 역설 확인\n",
    "    is_braess = all(new_time < initial_time for new_time, initial_time in zip(new_times, initial_times))\n",
    "    if is_braess:\n",
    "        print(f\"링크 {link['LINK_ID']}에서 Braess의 역설이 관찰되었습니다.\")\n",
    "        braess_paradox_links.append(link['LINK_ID'])\n",
    "\n",
    "    # 결과 저장\n",
    "    for i, (path, initial_time, new_time) in enumerate(zip(alternative_paths, initial_times, new_times)):\n",
    "        results.append({\n",
    "            'Link_ID': link['LINK_ID'],\n",
    "            'Alternative_Path': ' -> '.join(map(str, path)),\n",
    "            'Initial_Time': initial_time,\n",
    "            'New_Time': new_time,\n",
    "            'Time_Difference': new_time - initial_time,\n",
    "            'Is_Braess': is_braess\n",
    "        })\n",
    "\n",
    "    # 그래프에 링크 다시 추가\n",
    "    G.add_edge(start_node, end_node,\n",
    "               length=link['LENGTH'],\n",
    "               speed=link['New_Speed'],\n",
    "               link_id=link['LINK_ID'],\n",
    "               capacity=link['C_U'],\n",
    "               bpr_a=link['BPR_A'],\n",
    "               bpr_b=link['BPR_B'])\n",
    "\n",
    "print(f\"Braess의 역설을 보이는 링크 수: {len(braess_paradox_links)}\")\n",
    "print(\"Braess의 역설을 보이는 링크:\", braess_paradox_links)\n",
    "\n",
    "# 결과를 DataFrame으로 변환\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# CSV 파일로 저장\n",
    "results_df.to_csv('braess_paradox_results.csv', index=False, encoding='utf-8-sig')\n",
    "print(\"결과가 'braess_paradox_results.csv' 파일로 저장되었습니다.\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "# 표시할 LINK_ID 목록 정의\n",
    "link_ids_to_display = ['1170017402']#, '1170040400', '1170040500', '1170005202', '1170017402']\n",
    "\n",
    "# slink_df에서 해당 LINK_ID를 필터링\n",
    "filtered_links = slink_df[slink_df['LINK_ID'].isin(link_ids_to_display)]\n",
    "\n",
    "# 특정 위치를 중심으로 Folium 지도 생성\n",
    "m = folium.Map(location=[37.5665, 126.9780], zoom_start=15)\n",
    "\n",
    "# 필터링된 링크를 지도에 추가\n",
    "for _, row in filtered_links.iterrows():\n",
    "    if isinstance(row['geometry'], MultiLineString) and len(row['geometry'].geoms) > 1:\n",
    "        for line_string in row['geometry'].geoms:\n",
    "            coordinates = [(lat, lon) for lon, lat in line_string.coords]\n",
    "            folium.PolyLine(\n",
    "                locations=coordinates,\n",
    "                color='red',\n",
    "                popup=f\"LINK_ID: {row['LINK_ID']}\"\n",
    "            ).add_to(m)\n",
    "    else:\n",
    "        coordinates = [(lat, lon) for lon, lat in zip(*row['geometry'].xy)]\n",
    "        folium.PolyLine(\n",
    "            locations=coordinates,\n",
    "            color='red',\n",
    "            popup=f\"LINK_ID: {row['LINK_ID']}\"\n",
    "        ).add_to(m)\n",
    "        \n",
    "# 지도 저장\n",
    "m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
